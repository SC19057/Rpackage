data("law",package = "bootstrap")
x <- as.matrix(law)
xrange <- seq(from = 500, to = 700, by = 10)
yrange <- seq(from = 1, to = 4, by = 0.05)
k <- 4
fit <- knnde(x, k, xrange, yrange)
persp(xrange,yrange,fit,col='orange')
devtools::document()
data("law")
x <- law$LSAT
y <- law$GPA
B <- 20
indTest(x,y,B,0.05)
set.seed(123)
m<-1e4
t<-runif(m,0,pi/3)
int.m<-mean(sin(t))*pi/3
int.e<-cos(0)-cos(pi/3)
cat("Monte Carlo estimate=",int.m)
cat("\nexact value=",int.e)
integ<-function(m)
{set.seed(123)
t<-runif(m,0,pi/3)
int.m<-mean(sin(t))*pi/3
return(int.m)
}
integ(1e5)
integ(1e6)
integ(1e7)
devtools::install_github("SC19089/Package")
devtools::install_github("SC19089/Package")
library(SC19089)
Brown(2,7)
install.packages("e1071")
devtools::install_github("SC19089/Package")
library(SC19089)
Brown(2,7)
sseR(8)
sseR(8)
t=numeric(1e30)
for(iin 1:1e3){
t[i]=xtR(100,1)
}
for(i in 1:1e3){
t[i]=xtR(100,1)
}
t
devtools::install_github("SC19089/Package")
devtools::install_github("SC19089/Package")
devtools::install_github("SC19089/Package")
x<-rnorm(150)
y1<-rt(150,50)
y2<-rt(150,100)
y3<-rt(150,150)
layout(matrix(c(1,1,1,2,3,4),2,3,byrow = TRUE))
hist(x,main="The Distribution Of X",col.main="red",col="blue",xlim =c(-3,3))
hist(y1,main="The Distribution Of y1",col.main="orange",col="green",xlim =c(-3,3))
hist(y2,main="The Distribution Of y2",col.main="orange",col="green",xlim =c(-3,3))
hist(y3,main="The Distribution Of y3",col.main="orange",col="green",xlim =c(-3,3))
library(knitr)
library(DAAG)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
# cross validation
e1 <- e2 <- e3 <- e4 <- numeric(n)
for (k in 1:n) {
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2]*chemical[k]
e1[k] <- magnetic[k] - yhat1
J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2]*chemical[k] +J2$coef[3]*chemical[k]^2
e2[k] <- magnetic[k] - yhat2
J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2]*chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3
J4 <- lm(y ~ x + I(x^2)+I(x^3))
yhat4 <- J4$coef[1] + J4$coef[2]*chemical[k] + J4$coef[3]*chemical[k]^2 + J4$coef[4]*chemical[k]^3
e4[k] <- magnetic[k] - yhat4
}
L1 <- lm(magnetic~chemical)
L2 <- lm(magnetic~chemical+I(chemical^2))
L3 <- lm(log(magnetic)~chemical)
L4 <- lm(magnetic~chemical+I(chemical^2)+I(chemical^3))
# to get adjustied R^2
r1<-summary(L1)$adj.r.squared
r2<-summary(L2)$adj.r.squared
r3<-summary(L3)$adj.r.squared
r4<-summary(L4)$adj.r.squared
results<-data.frame(Linear=c(mean(e1^2),r1),Quadratic=c(mean(e2^2),r2),
Exponential=c(mean(e3^2),r3),Cubic=c(mean(e4^2),r4),
row.names=c(" prediction error","adjustied R-square"))
kable(results)
knitr::opts_chunk$set(echo = TRUE)
library(DAAG)
attach(ironslag)
set.seed(12345)
n = length(magnetic)
e1 = e2 = e3 = e4 = numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for(k in 1:n){
y = magnetic[-k]
x = chemical[-k]
J1 = lm(y~x)
yhat1 = J1$coef[1]+J1$coef[2]*chemical[k]
e1[k] = magnetic[k]-yhat1
J2 = lm(y~x+I(x^2))
yhat2 = J2$coef[1]+J2$coef[2]*chemical[k]+J2$coef[3]*chemical[k]^2
e2[k] = magnetic[k]-yhat2
J3 = lm(log(y)~x)
logyhat3 = J3$coef[1]+J3$coef[2]*chemical[k]
yhat3 = exp(logyhat3)
e3[k] = magnetic[k]-yhat3
J4 = lm(y~x+I(x^2)+I(x^3))
yhat4 = J4$coef[1]+J4$coef[2]*chemical[k]+J4$coef[3]*chemical[k]^2+J4$coef[4]*chemical[k]^3
e4[k] = magnetic[k]-yhat4
}
c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2))
L1 = lm(magnetic~chemical)
L2 = lm(magnetic~chemical+I(chemical^2))
L3 = lm(log(magnetic)~chemical)
L4 = lm(magnetic~chemical+I(chemical^2)+I(chemical^3))
round(c(summary(L1)$adj.r.squared,summary(L2)$adj.r.squared,summary(L3)$adj.r.squared,summary(L4)$adj.r.squared),4)
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
set.seed(123)
# Count Five test permutation
count5test_permutation <- function(z) {
n <- length(z)
x <- z[1:(n/2)]
y <- z[-(1:(n/2))]
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
permutation <- function(z,R) {
n <- length(z)
out <- numeric(R)
for (r in 1: R){
p <- sample(1:n ,n ,replace = FALSE)
out[r] <- count5test_permutation(z[p])
}
sum(out)/R
}
n1 <- 20
n2 <- 50
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 1e3
alphahat1 <- mean(replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)
count5test(x, y)
}))
alphahat2 <- mean(replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)
z <- c(x,y)
permutation(z,1000)
})<0.05)
round(c(count5test=alphahat1,count5test_permutation=alphahat2),4)
#some function used in distance correlation
dCov <- function(x, y) {
x <- as.matrix(x); y <- as.matrix(y)
n <- nrow(x); m <- nrow(y)
if (n != m || n < 2) stop("Sample sizes must agree")
if (! (all(is.finite(c(x, y)))))
stop("Data contains missing or infinite values")
Akl <- function(x) {
d <- as.matrix(dist(x))
m <- rowMeans(d); M <- mean(d)
a <- sweep(d, 1, m); b <- sweep(a, 2, m)
b + M
}
A <- Akl(x); B <- Akl(y)
sqrt(mean(A * B))
}
ndCov2 <- function(z, ix, dims) {
#dims contains dimensions of x and y
p <- dims[1]
q <- dims[2]
d <- p + q
x <- z[ , 1:p] #leave x as is
y <- z[ix, -(1:p)] #permute rows of y
return(nrow(z) * dCov(x, y)^2)
}
library(boot)
library(Ball)
set.seed(123)
n<-20
p.cor1<-p.cor2<-p.ball1<-p.ball2<-numeric(n)
for(i in 1:n){
x <- e <- matrix(rnorm(2*i), , 2)
# for model 1
y1<-x/4+e
z1 <- cbind(x,y1)
boot.obj1 <- boot(data = z1, statistic = ndCov2, R = 999,
sim = "permutation", dims = c(2, 2))
# permutatin: resampling without replacement
tb1 <- c(boot.obj1$t0, boot.obj1$t)
p.cor1[i]<- 1-mean(tb1>=tb1[1])
#ball
p.ball1 [i]<- 1-bcov.test(z1[,1:2],z1[,3:4],R=999)$p.value
# for model 2
y2<-x/4*e
z2 <- cbind(x,y2)
boot.obj2 <- boot(data = z2, statistic = ndCov2, R = 999,
sim = "permutation", dims = c(2, 2))
# permutatin: resampling without replacement
tb2 <- c(boot.obj2$t0, boot.obj2$t)
p.cor2[i]<- 1-mean(tb2>=tb2[1])
#ball
p.ball2[i] <- 1-bcov.test(z2[,1:2],z2[,3:4],R=999)$p.value
}
plot(1:20,p.cor1,'l',xlab = 'n',ylab = 'power',main = 'model 1')
lines(1:20,p.ball1,'l',col='red')
legend(10,0.4,c('distance correlation test','ball covariance test'),col=c('black','red'),
lty = c(1,1),cex = 0.7 )
plot(1:20,p.cor2,'l',xlab = 'n',ylab = 'power',main = 'model 2')
lines(1:20,p.ball2,'l',col='red')
legend(10,0.4,c('distance correlation test','ball covariance test'),col=c('black','red'),
lty = c(1,1),cex = 0.7 )
library(parallel)
num_cores = detectCores()
cluster = makePSOCKcluster(num_cores)
mcsapply = function(cluster,X,FUN,...){
res = parLapply(cluster,X,FUN,...)
simplify2array(res)
}
system.time(mcsapply(cluster, 1:10, function(i) Sys.sleep(1)))
system.time(sapply(1:10, function(i) Sys.sleep(1)))
stopCluster(cluster)
set.seed(1234)
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n,3,1)
x<-function(p1){
p<-p1
u <- runif(n)
k <- as.integer(u < p) #vector of 0’s and 1’s
x <- k * x1 + (1-k) * x2
}#generte a mixture sample
layout(matrix(c(1:6),2,3,byrow = TRUE))
hist(x(0.75), prob=TRUE);lines(density(x(0.75)),col="red")
hist(x(0.15), prob=TRUE);lines(density(x(0.15)),col="red")
hist(x(0.3), prob=TRUE);lines(density(x(0.3)),col="red")
hist(x(0.45), prob=TRUE);lines(density(x(0.45)),col="red")
hist(x(0.6), prob=TRUE);lines(density(x(0.6)),col="red")
hist(x(0.9), prob=TRUE);lines(density(x(0.9)),col="red")
set.seed(1234)
x<-function(a){
shape<-1/(2*a^2)
t<-rgamma(1e4,1,shape)
x<-sqrt(t)
return(x)
}#generate a random R(σ) sample
layout(matrix(c(1:4),2,2,byrow = TRUE))
hist(x(0.1),prob=TRUE)
hist(x(0.5),prob=TRUE)
hist(x(2),prob=TRUE)
hist(x(5),prob=TRUE)
x<-rnorm(150)
y1<-rt(150,50)
y2<-rt(150,100)
y3<-rt(150,150)
layout(matrix(c(1,1,1,2,3,4),2,3,byrow = TRUE))
hist(x,main="The Distribution Of X",col.main="red",col="blue",xlim =c(-3,3))
hist(y1,main="The Distribution Of y1",col.main="orange",col="green",xlim =c(-3,3))
hist(y2,main="The Distribution Of y2",col.main="orange",col="green",xlim =c(-3,3))
hist(y3,main="The Distribution Of y3",col.main="orange",col="green",xlim =c(-3,3))
set.seed(12345)
wishart <- function(n,sigma){
d <- nrow(sigma)
T <- matrix(rep(0,d*d),d,d)
for(i in 1:d){
for(j in 1:i){
if(i>j) T[i,j] <- rnorm(1)
if(i==j) T[i,j] <- sqrt(rchisq(1,df = (n-i+1)))
}
}
L <- t(chol(sigma))
M <- (L%*%T)%*%t(L%*%T)
M
}
n <- 5
sigma <- matrix(c(1,0.7,0.7,1), nrow = 2, ncol = 2)
d <- 2
M <- wishart(n,sigma)
M
set.seed(123)
m<-1e4
t<-runif(m,0,pi/3)
int.m<-mean(sin(t))*pi/3
int.e<-cos(0)-cos(pi/3)
cat("Monte Carlo estimate=",int.m)
cat("\nexact value=",int.e)
set.seed(123)
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
alpha <- .1
n <- 30
m <- 2500
epsilon <- seq(0,1, .01)
N <- length(epsilon)
par(cex=0.7,cex.lab=1.5,cex.axis=1,cex.main=2)
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))#critical value for the skewness test
# against beta distribution
a<-seq(0,5,0.05)
pw<-numeric(N)
for (j in 1:N) { #for each a
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
x <- rbeta(n,a[j],5)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pw[j] <- mean(sktests)
}
plot(a, pw, type = "b",xlab = bquote(a), ylim = c(0,1),main='against beta distribution')
#contaminated beta distribution
pw.b<-numeric(N)
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
s <- sample(c(4, 1000), replace = TRUE,
size = n, prob = c(1-e, e))
x <- rbeta(n,s,s)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pw.b[j] <- mean(sktests)
}
plot(epsilon, pw.b, type = "b",
xlab = bquote(epsilon), ylim = c(0,1),main = 'against contaminated beta distribution')
#against t(v)
pw.t <- numeric(N)
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
d.f <- sample(c(1, 5), replace = TRUE,
size = n, prob = c(1-e, e))
x <- rt(n,d.f)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pw.t[j] <- mean(sktests)
}
plot(epsilon, pw.t, type = 'p',pch='$',col='goldenrod',
xlab = bquote(epsilon), ylim = c(0,1),main = 'aganist t-distribution')
#some function used in distance correlation
dCov <- function(x, y) {
x <- as.matrix(x); y <- as.matrix(y)
n <- nrow(x); m <- nrow(y)
if (n != m || n < 2) stop("Sample sizes must agree")
if (! (all(is.finite(c(x, y)))))
stop("Data contains missing or infinite values")
Akl <- function(x) {
d <- as.matrix(dist(x))
m <- rowMeans(d); M <- mean(d)
a <- sweep(d, 1, m); b <- sweep(a, 2, m)
b + M
}
A <- Akl(x); B <- Akl(y)
sqrt(mean(A * B))
}
ndCov2 <- function(z, ix, dims) {
#dims contains dimensions of x and y
p <- dims[1]
q <- dims[2]
d <- p + q
x <- z[ , 1:p] #leave x as is
y <- z[ix, -(1:p)] #permute rows of y
return(nrow(z) * dCov(x, y)^2)
}
library(boot)
library(Ball)
set.seed(123)
n<-20
p.cor1<-p.cor2<-p.ball1<-p.ball2<-numeric(n)
for(i in 1:n){
x <- e <- matrix(rnorm(2*i), , 2)
# for model 1
y1<-x/4+e
z1 <- cbind(x,y1)
boot.obj1 <- boot(data = z1, statistic = ndCov2, R = 999,
sim = "permutation", dims = c(2, 2))
# permutatin: resampling without replacement
tb1 <- c(boot.obj1$t0, boot.obj1$t)
p.cor1[i]<- 1-mean(tb1>=tb1[1])
#ball
p.ball1 [i]<- 1-bcov.test(z1[,1:2],z1[,3:4],R=999)$p.value
# for model 2
y2<-x/4*e
z2 <- cbind(x,y2)
boot.obj2 <- boot(data = z2, statistic = ndCov2, R = 999,
sim = "permutation", dims = c(2, 2))
# permutatin: resampling without replacement
tb2 <- c(boot.obj2$t0, boot.obj2$t)
p.cor2[i]<- 1-mean(tb2>=tb2[1])
#ball
p.ball2[i] <- 1-bcov.test(z2[,1:2],z2[,3:4],R=999)$p.value
}
plot(1:20,p.cor1,'l',xlab = 'n',ylab = 'power',main = 'model 1')
lines(1:20,p.ball1,'l',col='red')
legend(10,0.4,c('distance correlation test','ball covariance test'),col=c('black','red'),
lty = c(1,1),cex = 0.7 )
plot(1:20,p.cor2,'l',xlab = 'n',ylab = 'power',main = 'model 2')
lines(1:20,p.ball2,'l',col='red')
legend(10,0.4,c('distance correlation test','ball covariance test'),col=c('black','red'),
lty = c(1,1),cex = 0.7 )
#define the density fuction
dl<-function(x) 1/2*exp(-abs(x))
rw.Metropolis <- function(n, sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (dl(y) / dl(x[i-1])))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}
n <- 4
N <- 3000
sigma <- c(.01, .1, 1, 10)
x0 <- 5
index <- 0:N
layout(matrix(c(1:4),2,2,byrow = TRUE))
#sigama=0.01
rw1 <- rw.Metropolis(n, sigma[1], x0, N)
y1 <- rw1$x[index]
plot(y1, type="l", main=expression(sigma==0.01), ylab="x",xlab = "")
p1<-1-rw1$k/N
#sigama=0.1
rw2 <- rw.Metropolis(n, sigma[2], x0, N)
y2 <- rw2$x[index]
plot(y2, type="l", main=expression(sigma==0.1), ylab="x",xlab = "")
p2<-1-rw2$k/N
#sigama=1
rw3 <- rw.Metropolis(n, sigma[3], x0, N)
y3 <- rw3$x[index]
plot(y3, type="l", main=expression(sigma==1), ylab="x",xlab = "")
p3<-1-rw3$k/N
#sigma=10
rw4 <- rw.Metropolis(n, sigma[4], x0, N)
y4 <- rw4$x[index]
plot(y4, type="l", main=expression(sigma==10), ylab="x",xlab = "")
p4<-1-rw4$k/N
rate<-data.frame(p1,p2,p3,p4,row.names ='acceptance rate')
names(rate)<-c(0.01,0.1,1,10)
knitr::kable(rate)
devtools::build_vignettes()
devtools::document()
library(SC19057)
